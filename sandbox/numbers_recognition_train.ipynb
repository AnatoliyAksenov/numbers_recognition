{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import IPython\n",
    "import os\n",
    "import uuid\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"c:/numbers_recognition/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import librosa.core\n",
    "import librosa.display\n",
    "import librosa.effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "import random\n",
    "import numpy.random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.preprocessing.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ds_complete(file):\n",
    "    label, gender, media = [], [], []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            l,g,m = line.split(',')\n",
    "            label.append(l.strip())\n",
    "            gender.append(g.strip())\n",
    "            media.append(m.strip())\n",
    "    return label, gender, media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras Sequence dataset'\n",
    "    \n",
    "    def __init__(self, files, labels, path=\"wav\", samples=10, batch_size=16, maxlen=5e5, n_classes=30, shuffle=True):\n",
    "        'Initialize class'\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # labels\n",
    "        self.labels = labels\n",
    "        \n",
    "        # files\n",
    "        self.files = files\n",
    "        self.csvs = [os.path.join(path, re.findall(\"(\\w+-\\w+-\\w+-\\w+-\\w+)\\.dat\", x)[0]+\".wav\") for x in files]\n",
    "        self.uuids = [re.findall(\"(\\w+-\\w+-\\w+-\\w+-\\w+)\\.dat\", x)[0] for x in files]\n",
    "        self.label_by_file = dict(zip(files, labels))\n",
    "        self.label_by_uuid = dict(zip(self.uuids, labels))\n",
    "        \n",
    "        # params\n",
    "        self.path = path\n",
    "        self.maxlen = int(maxlen)\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.samples = samples\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the number of batches per epoch'\n",
    "        return self.samples\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        files = [self.files[random.randint(0, len(self.files)-1)] for x in range(self.batch_size)]\n",
    "        X, y = self.__data_generation(files)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def __data_generation(self, files):\n",
    "        'Generates data containing batch_size samples' \n",
    "        \n",
    "        X = np.empty((self.batch_size, self.maxlen))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, file in enumerate(files):\n",
    "            # load file\n",
    "            # MONO rate=8k auto trimming\n",
    "            fn = os.path.join(self.path, re.findall(\"(\\w+-\\w+-\\w+-\\w+-\\w+)\\.dat\", file)[0]+\".wav\")\n",
    "            data, fs = librosa.load(fn, mono=True, sr=8e3)\n",
    "            trimed, index = librosa.effects.trim(data, top_db=29, frame_length=10)\n",
    "            \n",
    "            if self.shuffle:\n",
    "                rs = random.randint(100,1000)\n",
    "                rss = random.randint(5000,10000)\n",
    "\n",
    "                smp = np.concatenate( (np.random.ranf(size=rs)/1e3 , trimed[rs:], np.random.ranf(size=rss)/1e3), axis=0)\n",
    "            else:\n",
    "                smp = trimed\n",
    "            \n",
    "            X[i,] = np.concatenate( (smp[: min(self.maxlen, len(smp))], np.zeros(( self.maxlen - min(self.maxlen, len(smp))))), axis=0)\n",
    "\n",
    "            # Store class\n",
    "            y[i] = tokenizer.word_index[str(self.label_by_file[file])]\n",
    "            # print(y)\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes) # Why? need + 1 ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = glob.glob(\"dataset/*.completed.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_to_class = {}\n",
    "uuid_to_class = {}\n",
    "file_to_gender = {}\n",
    "uuid_to_gender = {}\n",
    "files = []\n",
    "labels = []\n",
    "gender = []\n",
    "\n",
    "for f in csv:\n",
    "    l,g,m = ds_complete(f)\n",
    "    files += m\n",
    "    labels += l\n",
    "    gender += g\n",
    "    file_to_class.update({\"csv/\"+re.findall(\"(\\w+-\\w+-\\w+-\\w+-\\w+)\\.dat\", x)[0]+\".csv\":l[i] for i,x in enumerate(m)})\n",
    "    uuid_to_class.update({re.findall(\"(\\w+-\\w+-\\w+-\\w+-\\w+)\\.dat\", x)[0]:l[i] for i,x in enumerate(m)})\n",
    "    file_to_gender.update({\"csv/\"+re.findall(\"(\\w+-\\w+-\\w+-\\w+-\\w+)\\.dat\", x)[0]+\".csv\":g[i] for i,x in enumerate(m)})\n",
    "    uuid_to_gender.update({re.findall(\"(\\w+-\\w+-\\w+-\\w+-\\w+)\\.dat\", x)[0]:g[i] for i,x in enumerate(m)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid_to_class['058f821c-d87b-11e8-997b-f48c5031df2f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 45)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=len({x:x for x in labels})\n",
    "classes = classes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length = int(1e4)\n",
    "embedding=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataGenerator(files=files, labels=labels, samples=100, maxlen=max_review_length, batch_size=16, shuffle=True, n_classes=classes)\n",
    "test = DataGenerator(files=files, labels=labels, samples=10, maxlen=max_review_length, batch_size=16, shuffle=False, n_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 10000, 100)        1000000   \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 10000, 125)        62625     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5000, 125)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 5000, 32)          12032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 2500, 16)          1552      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 1250, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1250, 16)          0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100)               46800     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 22)                2222      \n",
      "=================================================================\n",
      "Total params: 1,125,231\n",
      "Trainable params: 1,125,231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Embedding\n",
    "model.add(Embedding(max_review_length, embedding, input_length=max_review_length))\n",
    "\n",
    "# Convilution\n",
    "model.add(Conv1D(filters=125, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Dropout\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# LSTM\n",
    "model.add(LSTM(100))\n",
    "\n",
    "# Classification\n",
    "model.add(Dense(classes, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.3098 - acc: 0.9299 - val_loss: 0.1834 - val_acc: 0.9545\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 140s 3s/step - loss: 0.1792 - acc: 0.9545 - val_loss: 0.1769 - val_acc: 0.9545\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.1786 - acc: 0.9545 - val_loss: 0.1773 - val_acc: 0.9545\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 140s 3s/step - loss: 0.1794 - acc: 0.9545 - val_loss: 0.1784 - val_acc: 0.9545\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.1796 - acc: 0.9545 - val_loss: 0.1786 - val_acc: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3a90cb70>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=train, steps_per_epoch=50, epochs=5, validation_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"nr_5e_22c.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open(\"tokenizer_22.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
